{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11479730,"sourceType":"datasetVersion","datasetId":7195001}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Deep Learning Model Analysis, Discussion, and Conclusions\n\n### Models Used\n\nFor this project, I explored the task of analyzing and comparing songs based on their lyrical and metadata similarities. The goal was to identify which deep learning-based sentence embedding models best capture the overall \"vibe\" of a song, measured through a custom similarity function. I evaluated the following pre-trained transformer-based models from the `sentence-transformers` library:\n\n- `all-MiniLM-L12-v2`\n- `all-mpnet-base-v2`\n- `sentence-t5-base`\n\nThese models are widely used for sentence-level semantic similarity tasks and were chosen for their balance of performance and efficiency.\n\n---\n\n### Custom Similarity Function\n\nA novel aspect of the analysis was the development of a **custom similarity metric** that incorporates both deep semantic embeddings and structured metadata. The final similarity score between two songs was computed as a weighted average of:\n\n- Cosine similarity of **lyrics**\n- Cosine similarity of **titles**\n- Genre match\n- Artist match\n- Year similarity (modeled with an exponential decay)\n- View-based popularity score\n- Feature overlap (e.g., moods, themes)\n\nThis allowed the system to go beyond basic keyword or text similarity and consider a richer context around each song.\n\n---\n\n### Experimental Setup\n\nTo evaluate each model’s ability to identify semantically and contextually similar songs, I randomly generated 500 pairs of songs over 5 rounds (100 per round). For each pair, the similarity score was computed using the custom function, and average scores were recorded per model.\n\nThe evaluation was not supervised in a traditional classification or regression sense due to the lack of labeled ground truth for “similarity,” but it provided a comparative insight into the stability and sensitivity of each model in a real-world, unstructured use case.\n\n---\n\n### Results Summary\n\n| Model               | Mean Similarity | Std Deviation | Total Pairs |\n|--------------------|----------------:|--------------:|------------:|\n| all-MiniLM-L12-v2  |  0.2994    | 0.0939        | 500         |\n| all-mpnet-base-v2  |  0.3371    | 0.1042        | 500         |\n| sentence-t5-base   |  0.5908   | 0.0808        | 500         |\n\n\nFrom these results, `sentence-t5-base` slightly outperformed the others in terms of average similarity score and lower variance, suggesting it was more consistent in its representation of song content and metadata alignment.\n\n---\n\n### Search Engine Demo (Bonus)\n\nIn addition to the similarity analysis, I built a **console-based song search engine** that takes a user-defined phrase (or “vibe”) as input and returns the top 10 most relevant songs based on FAISS-powered semantic search. This component uses the precomputed song embeddings to efficiently return high-quality results in real time.\n\nTo keep the project lightweight and ensure faster experimentation, I limited the dataset to **100,000 songs** sampled from the full corpus. For the search engine backend, I selected the **`all-MiniLM-L12-v2`** model. Although this model did not perform as strongly in the evaluation compared to alternatives like `sentence-t5-base`, it offered **significantly faster encoding times and lower memory usage**, making it ideal for prototyping and real-time search applications.\n\n\n**Sample use cases:**\n\n- Entering `heartbreak` retrieves emotional, slow-paced tracks.\n- Entering `hype` brings back upbeat, high-energy songs.\n\nThis demo helps demonstrate the practical value of the embeddings and similarity models used in the main analysis.\n\n---\n\n### Discussion & Insights\n\n- **Model Strengths**: All three models performed reasonably well, though `sentence-t5-base` offered slightly stronger and more stable results, likely due to its higher parameter count and deeper architecture.\n- **Limitations**: The custom similarity function involves multiple handcrafted weightings, which could be further optimized with labeled data or learned end-to-end. Also, there's no objective “ground truth” for similarity without user studies or annotations.\n- **Hyperparameter Considerations**: While deep learning models were used as-is, the primary tuning effort involved adjusting weights in the similarity function to reflect reasonable assumptions (e.g., lyrics > titles).\n- **Scalability**: The system scales well due to FAISS indexing, even on large datasets (tested with 100,000+ rows), and benefits from GPU acceleration.\n\n---","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Read the CSV file from the working directory\ndf_clean = pd.read_csv('/kaggle/input/df-clean/df_clean_output.csv')\ndf_clean = df_clean.sample(n=100000, random_state=42).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T00:21:05.116384Z","iopub.execute_input":"2025-04-21T00:21:05.117062Z","iopub.status.idle":"2025-04-21T00:24:51.023947Z","shell.execute_reply.started":"2025-04-21T00:21:05.117004Z","shell.execute_reply":"2025-04-21T00:24:51.023382Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom sentence_transformers import SentenceTransformer\nimport random\n\n# ----------- 1. Cosine similarity helper -----------\ndef cosine_sim(model, text1, text2):\n    emb1 = model.encode(text1, convert_to_tensor=True, show_progress_bar=False, disable_tqdm=True)\n    emb2 = model.encode(text2, convert_to_tensor=True, show_progress_bar=False, disable_tqdm=True)\n    return float(torch.nn.functional.cosine_similarity(emb1, emb2, dim=0))\n\n# ----------- 2. Custom similarity function -----------\ndef custom_similarity(song1, song2, model=None, weights=None):\n    if weights is None:\n        weights = {\n            'lyric_sim': 0.4,\n            'title_sim': 0.1,\n            'genre_match': 0.1,\n            'artist_match': 0.1,\n            'year_score': 0.1,\n            'view_score': 0.1,\n            'feature_match': 0.1\n        }\n\n    def safe_get(s, key, default=\"\"):\n        return s.get(key, default) if pd.notna(s.get(key, default)) else default\n\n    lyric_sim = cosine_sim(model, str(safe_get(song1, 'clean_lyrics')), str(safe_get(song2, 'clean_lyrics')))\n    title_sim = cosine_sim(model, str(safe_get(song1, 'title')), str(safe_get(song2, 'title')))\n    genre_match = 1.0 if safe_get(song1, 'tag') == safe_get(song2, 'tag') else 0.0\n    artist_match = 1.0 if safe_get(song1, 'artist') == safe_get(song2, 'artist') else 0.0\n\n    try:\n        year1 = int(float(safe_get(song1, 'year', 0)))\n        year2 = int(float(safe_get(song2, 'year', 0)))\n    except:\n        year1, year2 = 0, 0\n    year_diff = abs(year1 - year2)\n    year_score = np.exp(-year_diff / 10.0)\n\n    try:\n        views1 = int(float(safe_get(song1, 'views', 1)))\n        views2 = int(float(safe_get(song2, 'views', 1)))\n    except:\n        views1, views2 = 1, 1\n    max_views = max(views1, views2, 1)\n    view_score = min(views1, views2) / max_views\n\n    features1 = set(safe_get(song1, 'clean_features', \"\").split(\", \")) if isinstance(song1.get('clean_features'), str) else set()\n    features2 = set(safe_get(song2, 'clean_features', \"\").split(\", \")) if isinstance(song2.get('clean_features'), str) else set()\n    feature_match = len(features1 & features2) / max(len(features1), len(features2), 1)\n\n    final_score = (\n        weights['lyric_sim'] * lyric_sim +\n        weights['title_sim'] * title_sim +\n        weights['genre_match'] * genre_match +\n        weights['artist_match'] * artist_match +\n        weights['year_score'] * year_score +\n        weights['view_score'] * view_score +\n        weights['feature_match'] * feature_match\n    )\n    return final_score\n\n# ----------- 3. Load models -----------\nmodel_names = ['all-MiniLM-L12-v2', 'all-mpnet-base-v2', 'sentence-t5-base']\nmodels = {name: SentenceTransformer(f'sentence-transformers/{name}') for name in model_names}\n\n# ----------- 4. Generate song pairs -----------\ndef generate_song_pairs(df, n_pairs=100):\n    indices = list(df.index)\n    pairs = []\n    for _ in range(n_pairs):\n        i, j = random.sample(indices, 2)\n        pairs.append((df.loc[i], df.loc[j]))\n    return pairs\n\n# ----------- 5. Evaluate models on song pairs -----------\ndef evaluate_custom_similarity(models, df, n_pairs=100):\n    song_pairs = generate_song_pairs(df, n_pairs)\n    results = {}\n\n    for name, model in models.items():\n        print(f\"Evaluating model: {name}\")\n        sim_scores = []\n        for song1, song2 in song_pairs:\n            try:\n                score = custom_similarity(song1, song2, model=model)\n                sim_scores.append(score)\n            except Exception as e:\n                print(f\"Error comparing songs: {e}\")\n                sim_scores.append(None)\n        results[name] = sim_scores\n    return results\n\n# ----------- 6. Multi-round Evaluation -----------\nn_rounds = 5\nn_pairs_per_round = 100\naggregated_scores = {name: [] for name in models.keys()}\n\nfor round_num in range(n_rounds):\n    print(f\"\\n=== Round {round_num + 1} of {n_rounds} ===\")\n    round_results = evaluate_custom_similarity(models, df_clean, n_pairs=n_pairs_per_round)\n    for name, scores in round_results.items():\n        scores_filtered = [s for s in scores if s is not None]\n        aggregated_scores[name].extend(scores_filtered)\n\n# ----------- 7. Final Results -----------\nprint(\"\\n===== Average Custom Similarity per Model (Across Rounds) =====\")\nfor name, scores in aggregated_scores.items():\n    mean_sim = np.mean(scores)\n    std_sim = np.std(scores)\n    print(f\"{name}: Mean = {mean_sim:.4f} | Std = {std_sim:.4f} | Total Pairs = {len(scores)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T00:24:52.422444Z","iopub.execute_input":"2025-04-21T00:24:52.422729Z","iopub.status.idle":"2025-04-21T00:27:20.833545Z","shell.execute_reply.started":"2025-04-21T00:24:52.422712Z","shell.execute_reply":"2025-04-21T00:27:20.832905Z"}},"outputs":[{"name":"stderr","text":"2025-04-21 00:25:12.277418: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745195112.488405      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745195112.546774      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"576ca09d08264bd69ad7fab85efc65e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c35d0e8904234e5a8d7ac386de3fe988"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7965f68aa2949d3947e86958fe1d3e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed91f32d3ce749c6a2df64d3fb2c36a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4c8035217984c8ba469bea3e6932468"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17f26b2debb84d50bc86c61b7c27a6b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42041dcbf1f447a0be33e4978da8763a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dad6ad6076c4ebcab5a122e01337c26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"833dafd2788e4e56b98f705da62d4629"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e363b2570d34905b1b78a4c8aba6262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c054b680f08d40b4878ffdae029752bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97b8768d72b445d1b60a8dd61179917d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37f5bd252968416ea8d1792c6526439b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9a9347371d942ea96502cd72047084e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e133a624b7146cbad5bb1aaacc8811d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27e1069512774bda9a54e7b6006836bf"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14ab6d6e155946c791c124b8bb520a0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94dc098fbce54b1fbdbdea077650a45d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bb6b88eda854a6dae3cb3bafb6e2428"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8abe1e235f684d78b73d022a854b710a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff34462fcdf44a37a1050cbbcc3d26b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acd59532012040f284be2dd89e495197"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93416469fca440f99ea4e29c6fb028b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad6c55d4a9e9469393c26e2f4be02082"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.78k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeea6052a5474481b82a45669dbeb43e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d253158160374bdc96f490cca5562ac9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"211a239b4dc44bf996e28fe2c5016bb1"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/219M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04b1e6f3fc9d401abf55cc6ef573c42a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a0719e872bc44c697e3245e12feae48"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13b221674aa242f888d91bb0ae789837"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dd6e1ae430a4de4b3a71758767c9802"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73fbc7ee44734e3d8da2fb195bb2eeb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07f5b6aa308a4aeea2c0753e6acc24db"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcefd0be44cb4bf6bd3a56ddc575912b"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c4651bcda284630b09d33dc7361624b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8be1f1a0844a4ad8ac3956d895c023fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"rust_model.ot:   0%|          | 0.00/2.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71745517531e4098a5c2af6a24ce4d10"}},"metadata":{}},{"name":"stdout","text":"\n=== Round 1 of 5 ===\nEvaluating model: all-MiniLM-L12-v2\nEvaluating model: all-mpnet-base-v2\nEvaluating model: sentence-t5-base\n\n=== Round 2 of 5 ===\nEvaluating model: all-MiniLM-L12-v2\nEvaluating model: all-mpnet-base-v2\nEvaluating model: sentence-t5-base\n\n=== Round 3 of 5 ===\nEvaluating model: all-MiniLM-L12-v2\nEvaluating model: all-mpnet-base-v2\nEvaluating model: sentence-t5-base\n\n=== Round 4 of 5 ===\nEvaluating model: all-MiniLM-L12-v2\nEvaluating model: all-mpnet-base-v2\nEvaluating model: sentence-t5-base\n\n=== Round 5 of 5 ===\nEvaluating model: all-MiniLM-L12-v2\nEvaluating model: all-mpnet-base-v2\nEvaluating model: sentence-t5-base\n\n===== Average Custom Similarity per Model (Across Rounds) =====\nall-MiniLM-L12-v2: Mean = 0.2994 | Std = 0.0939 | Total Pairs = 500\nall-mpnet-base-v2: Mean = 0.3371 | Std = 0.1042 | Total Pairs = 500\nsentence-t5-base: Mean = 0.5908 | Std = 0.0808 | Total Pairs = 500\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install faiss-gpu-cu12","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T00:27:20.834269Z","iopub.execute_input":"2025-04-21T00:27:20.834516Z","iopub.status.idle":"2025-04-21T00:27:26.624295Z","shell.execute_reply.started":"2025-04-21T00:27:20.834499Z","shell.execute_reply":"2025-04-21T00:27:26.623338Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting faiss-gpu-cu12\n  Downloading faiss_gpu_cu12-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (24.2)\nRequirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (12.4.127)\nRequirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (12.8.4.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->faiss-gpu-cu12) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->faiss-gpu-cu12) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2->faiss-gpu-cu12) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2->faiss-gpu-cu12) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2->faiss-gpu-cu12) (2024.2.0)\nDownloading faiss_gpu_cu12-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu-cu12\nSuccessfully installed faiss-gpu-cu12-1.10.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport torch\nimport faiss\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# ---------- 1. Load model and move to GPU if available ----------\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = SentenceTransformer(\"sentence-transformers/all-MiniLM-L12-v2\")\nmodel = model.to(device)\n\n# ---------- 2. Batch encode and combine lyric + title ----------\ndef precompute_embeddings(df_clean, model):\n    lyrics = df_clean['clean_lyrics'].fillna(\"\").astype(str).tolist()\n    titles = df_clean['title'].fillna(\"\").astype(str).tolist()\n\n    # Batch encode (use GPU + batching)\n    lyric_embeddings = model.encode(lyrics, batch_size=64, convert_to_tensor=True, device=device, show_progress_bar=True)\n    title_embeddings = model.encode(titles, batch_size=64, convert_to_tensor=True, device=device, show_progress_bar=True)\n\n    # Combine both (average)\n    combined = (lyric_embeddings + title_embeddings) / 2\n    return combined\n\nprint(\"Precomputing song embeddings...\")\ncombined_embeddings_tensor = precompute_embeddings(df_clean, model)\ncombined_embeddings = combined_embeddings_tensor.cpu().numpy().astype('float32')\n\n# ---------- 3. Build FAISS index ----------\nprint(\"Building FAISS index...\")\nfaiss.normalize_L2(combined_embeddings)  # Important for cosine similarity\nindex = faiss.IndexFlatIP(combined_embeddings.shape[1])\nindex.add(combined_embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T00:27:26.626795Z","iopub.execute_input":"2025-04-21T00:27:26.627210Z","iopub.status.idle":"2025-04-21T00:31:33.986010Z","shell.execute_reply.started":"2025-04-21T00:27:26.627188Z","shell.execute_reply":"2025-04-21T00:31:33.985212Z"}},"outputs":[{"name":"stdout","text":"Precomputing song embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1563 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5feccff47864f17bb52714bdaf8dc2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1563 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"797d808b9cc6421c869c67adecaa2352"}},"metadata":{}},{"name":"stdout","text":"Building FAISS index...\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ---------- 4. Search function ----------\ndef search_songs(query, model, index, df_clean, top_k=10):\n    query_emb = model.encode(query, convert_to_tensor=True, device=device)\n    query_emb = query_emb.cpu().numpy().astype('float32').reshape(1, -1)\n    faiss.normalize_L2(query_emb)\n\n    D, I = index.search(query_emb, top_k)\n    results = df_clean.iloc[I[0]].copy()\n    results['score'] = D[0]\n    return results\n\n# ---------- 5. Console-based search (Kaggle-friendly) ----------\ndef song_search_console(df_clean):\n    while True:\n        query = input(\"\\nEnter a vibe to search for (or type 'exit' to quit): \").strip()\n        if query.lower() == 'exit':\n            print(\"Goodbye! \")\n            break\n        if query:\n            print(f\"\\nSearching for songs similar to: '{query}'\")\n            top_matches = search_songs(query, model, index, df_clean)\n\n            print(\"\\nTop Matching Songs:\")\n            for _, row in top_matches.iterrows():\n                print(f\"\\n**{row['title']}** by *{row['artist']}*\")\n                print(f\"Score: {row['score']:.3f} | Year: {row['year']} | Genre: {row['tag']}\")\n                print(\"-\" * 40)\n\n#  Run it\nsong_search_console(df_clean)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T00:31:33.986912Z","iopub.execute_input":"2025-04-21T00:31:33.987154Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"\nEnter a vibe to search for (or type 'exit' to quit):  motivation\n"},{"name":"stdout","text":"\nSearching for songs similar to: 'motivation'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f54178535c14dfb89155f4b8eb70998"}},"metadata":{}},{"name":"stdout","text":"\nTop Matching Songs:\n\n**Motivation** by *Ashland*\nScore: 0.815 | Year: 2019 | Genre: pop\n----------------------------------------\n\n**Motivation** by *Rivets*\nScore: 0.795 | Year: 2015 | Genre: pop\n----------------------------------------\n\n**Motivation** by *Dommyy*\nScore: 0.779 | Year: 2021 | Genre: rap\n----------------------------------------\n\n**Motivation** by *DeQuince*\nScore: 0.779 | Year: 2012 | Genre: rap\n----------------------------------------\n\n**Motivation Alternate Version** by *Dope*\nScore: 0.722 | Year: 2005 | Genre: pop\n----------------------------------------\n\n**Motivation** by *Shad da God*\nScore: 0.706 | Year: 2015 | Genre: rap\n----------------------------------------\n\n**Motivated** by *Sez Batters*\nScore: 0.698 | Year: 2011 | Genre: rap\n----------------------------------------\n\n**A Little Motivation** by *T-Bruin*\nScore: 0.646 | Year: 2013 | Genre: rap\n----------------------------------------\n\n**Greedy** by *Mia Stegner*\nScore: 0.598 | Year: 2020 | Genre: pop\n----------------------------------------\n\n**Inspiration** by *Kid Intelli*\nScore: 0.588 | Year: 2012 | Genre: rap\n----------------------------------------\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### Conclusion\n\nThis project successfully delivers on the goal of building a **vibe-based song search engine**—a system that goes beyond traditional filters like artist or genre by understanding and matching the *meaning* and *emotion* behind both user queries and song lyrics.\n\nBy leveraging **transformer-based sentence embeddings**, I was able to map lyrics and search phrases into a shared semantic space, enabling highly relevant matches based on themes and tone. The **custom similarity function** further enhanced search quality by incorporating additional metadata such as genre, artist, views, year, and featured artists.\n\nTo manage compute efficiently, I used a **sample of 100,000 rows** and the **`all-MiniLM-L12-v2`** model. While this model didn't achieve the highest similarity scores during evaluation, it trained significantly faster and allowed for rapid prototyping of the system. Despite its speed advantage, more powerful models like **`sentence-t5-base`** showed greater potential in terms of accuracy and could be integrated in future iterations.\n\nWith access to **more compute**, I could scale this project further by:\n\n- Processing the **entire dataset** of over 3 million songs  \n- Switching to **higher-performing models** like `sentence-t5-base`  \n- **Storing embeddings** in a persistent vector database for scalable, real-time search  \n\nThis work lays a solid foundation for expressive music discovery powered by modern NLP. \n\n**Future directions include:**\n\n- Fine-tuning embedding models on music-specific data  \n- Creating labeled training pairs for supervised similarity learning  \n- Deploying the system as a **fully interactive web app** (e.g., via Streamlit or Gradio)\n","metadata":{}}]}